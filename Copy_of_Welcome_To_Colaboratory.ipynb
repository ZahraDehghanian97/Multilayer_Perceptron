{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Welcome To Colaboratory",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhvNMdAYD8qf"
      },
      "source": [
        "# classification mlp model for the abalone dataset\n",
        "from numpy import unique\n",
        "from numpy import argmax\n",
        "from pandas import read_csv\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd  #**************\n",
        "import numpy as np  #**************\n",
        "from sklearn import preprocessing  #**************\n",
        "from tensorflow import keras   #**************\n",
        "import matplotlib.pyplot as plt  #**************\n",
        "from sklearn.metrics import confusion_matrix  #**************"
      ],
      "execution_count": 208,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGCgBrLeEPXn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5840444f-1b57-4d42-fc94-ec41590c0623"
      },
      "source": [
        "from google.colab import drive \n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CAIoRyVMsrro"
      },
      "source": [
        "#load dataset\n",
        "set_1 = pd.read_csv('/content/gdrive/MyDrive/NN/HW_2/dataSet.csv')\n",
        "set_1 = np.array(set_1)"
      ],
      "execution_count": 182,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWUqRs6ij3O7"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "#shuffling data\n",
        "List = np.random.permutation(len(set_1))\n",
        "dataset = []\n",
        "for i in range(len(set_1)):\n",
        "    dataset.append(set_1[List[i]-2])\n",
        "dataset =np.array(dataset)\n",
        "dataset = dataset[:10000]\n",
        "\n"
      ],
      "execution_count": 211,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "teoUWjP8HBPB",
        "outputId": "7dfca6a6-7255-487b-afec-5ef4654326a3"
      },
      "source": [
        "lable = dataset[:,0]\n",
        "feature = dataset[:,1: 91]\n",
        "Input_dim = feature.shape[1]\n",
        "y = LabelEncoder().fit_transform(lable)\n",
        "Output_dim = len(unique(y))\n",
        "\n",
        "#Split data to Test_data and Train_data \n",
        "X_train, X_test, y_train, y_test = train_test_split(feature, y, test_size=0.1, random_state=1)\n",
        "print(len(train),len(test))\n",
        "\n",
        "min_max_scaler = preprocessing.MinMaxScaler()\n",
        "X_train=min_max_scaler.fit_transform(X_train)\n",
        "X_test=min_max_scaler.transform(X_test)"
      ],
      "execution_count": 212,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9000 1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nasYg7EPNZ6g"
      },
      "source": [
        "model = keras.models.Sequential()\n",
        "model.add(keras.Input(shape=Input_dim))\n",
        "model.add(keras.layers.Dense(512, activation='relu', kernel_initializer='he_normal'))\n",
        "model.add(keras.layers.Dense(1000, activation='relu', kernel_initializer='he_normal'))\n",
        "model.add(keras.layers.Dense(Output_dim, activation='softmax'))\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEJih8DfO9rZ"
      },
      "source": [
        "# Compile model\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam',metrics=['accuracy'])"
      ],
      "execution_count": 223,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "okrGKNWzmJAr"
      },
      "source": [
        "# fit the keras model on the dataset\n",
        "history = model.fit(X_train, y_train, epochs=200, batch_size=50, verbose=1,validation_split=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTVOPSx4ES67"
      },
      "source": [
        "test_loss, test_acc = model.evaluate( X_test, y_test, verbose = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqQJ5XTxcbcD"
      },
      "source": [
        "# plot model accuracy and loss \n",
        "  # accuracy\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "# loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss (Number of neurons = 8)')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}